{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim=32\n",
    "height=32\n",
    "width=32\n",
    "channels=3\n",
    "\n",
    "generator_input=keras.Input(shape=(latent_dim,))\n",
    "x=layers.Dense(128*16*16)(generator_input)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Reshape((16,16,128))(x)\n",
    "x=layers.Conv2D(256,5,padding='same')(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2DTranspose(256,4,strides=2,padding='same')(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(256,5,padding='same')(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(256,5,padding='same')(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(channels,7,activation='tanh',padding='same')(x)\n",
    "generator=keras.models.Model(generator_input,x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input=layers.Input(shape=(height,width,channels))\n",
    "x=layers.Conv2D(128,3)(discriminator_input)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(128,4,strides=2)(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(128,4,strides=2)(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Conv2D(128,4,strides=2)(x)\n",
    "x=layers.LeakyReLU()(x)\n",
    "x=layers.Flatten()(x)\n",
    "x=layers.Dropout(0.4)(x)\n",
    "x=layers.Dense(1,activation='sigmoid')(x)\n",
    "discriminator=keras.models.Model(discriminator_input,x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer=keras.optimizers.RMSprop(lr=0.0008,clipvalue=1.0,decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_input=keras.Input(shape=(latent_dim,))\n",
    "gan_output=discriminator(generator(gan_input))\n",
    "gan=keras.models.Model(gan_input,gan_output)\n",
    "\n",
    "gan_optimizer=keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer,loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(_,_)=cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=y_train.flatten()==6\n",
    "index=[]\n",
    "for i in range(len(a)):\n",
    "    if a[i]==True:\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train[index]\n",
    "x_train=x_train.reshape((x_train.shape[0],)+(height,width,channels)).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucif\\Anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminator loss: 0.6839183\n",
      "\n",
      "Adverserial loss: 0.65192145\n",
      "\n",
      "Discriminator loss: 0.5966033\n",
      "\n",
      "Adverserial loss: 1.0690391\n",
      "\n",
      "Discriminator loss: 0.76113045\n",
      "\n",
      "Adverserial loss: 0.76456946\n",
      "\n",
      "Discriminator loss: 0.69549596\n",
      "\n",
      "Adverserial loss: 0.73518384\n",
      "\n",
      "Discriminator loss: 0.6878458\n",
      "\n",
      "Adverserial loss: 2.3250299\n",
      "\n",
      "Discriminator loss: 0.6891195\n",
      "\n",
      "Adverserial loss: 0.7491746\n",
      "\n",
      "Discriminator loss: 0.6974033\n",
      "\n",
      "Adverserial loss: 0.76907194\n",
      "\n",
      "Discriminator loss: 0.7002711\n",
      "\n",
      "Adverserial loss: 0.756145\n",
      "\n",
      "Discriminator loss: 0.69077426\n",
      "\n",
      "Adverserial loss: 0.722721\n",
      "\n",
      "Discriminator loss: 0.6955525\n",
      "\n",
      "Adverserial loss: 0.8556821\n",
      "\n",
      "Discriminator loss: 0.706803\n",
      "\n",
      "Adverserial loss: 0.85450995\n",
      "\n",
      "Discriminator loss: 0.7024051\n",
      "\n",
      "Adverserial loss: 0.7603878\n",
      "\n",
      "Discriminator loss: 0.69503766\n",
      "\n",
      "Adverserial loss: 0.7169459\n",
      "\n",
      "Discriminator loss: 0.6829109\n",
      "\n",
      "Adverserial loss: 0.76134723\n",
      "\n",
      "Discriminator loss: 0.710065\n",
      "\n",
      "Adverserial loss: 0.7339362\n",
      "\n",
      "Discriminator loss: 0.71664083\n",
      "\n",
      "Adverserial loss: 0.7738945\n",
      "\n",
      "Discriminator loss: 0.7021867\n",
      "\n",
      "Adverserial loss: 0.72818\n",
      "\n",
      "Discriminator loss: 0.71088517\n",
      "\n",
      "Adverserial loss: 0.75283265\n",
      "\n",
      "Discriminator loss: 0.6812916\n",
      "\n",
      "Adverserial loss: 0.74714726\n",
      "\n",
      "Discriminator loss: 0.6949166\n",
      "\n",
      "Adverserial loss: 0.80032283\n",
      "\n",
      "Discriminator loss: 0.692093\n",
      "\n",
      "Adverserial loss: 0.78720707\n",
      "\n",
      "Discriminator loss: 0.7134399\n",
      "\n",
      "Adverserial loss: 0.73862934\n",
      "\n",
      "Discriminator loss: 0.70110476\n",
      "\n",
      "Adverserial loss: 0.74188745\n",
      "\n",
      "Discriminator loss: 0.7084658\n",
      "\n",
      "Adverserial loss: 0.7495525\n",
      "\n",
      "Discriminator loss: 0.6954163\n",
      "\n",
      "Adverserial loss: 0.74809283\n",
      "\n",
      "Discriminator loss: 0.6958108\n",
      "\n",
      "Adverserial loss: 0.76455677\n",
      "\n",
      "Discriminator loss: 0.6775929\n",
      "\n",
      "Adverserial loss: 0.71040076\n",
      "\n",
      "Discriminator loss: 0.70401984\n",
      "\n",
      "Adverserial loss: 0.7919881\n",
      "\n",
      "Discriminator loss: 0.69733083\n",
      "\n",
      "Adverserial loss: 0.76142085\n",
      "\n",
      "Discriminator loss: 0.6825496\n",
      "\n",
      "Adverserial loss: 0.7529891\n",
      "\n",
      "Discriminator loss: 0.6899742\n",
      "\n",
      "Adverserial loss: 0.74767625\n",
      "\n",
      "Discriminator loss: 0.7098131\n",
      "\n",
      "Adverserial loss: 0.72011894\n",
      "\n",
      "Discriminator loss: 0.68043935\n",
      "\n",
      "Adverserial loss: 0.76621974\n",
      "\n",
      "Discriminator loss: 0.7093903\n",
      "\n",
      "Adverserial loss: 0.7783057\n",
      "\n",
      "Discriminator loss: 0.7048165\n",
      "\n",
      "Adverserial loss: 0.73872817\n",
      "\n",
      "Discriminator loss: 0.6970404\n",
      "\n",
      "Adverserial loss: 0.7758587\n",
      "\n",
      "Discriminator loss: 0.6893855\n",
      "\n",
      "Adverserial loss: 0.7825205\n",
      "\n",
      "Discriminator loss: 0.7024921\n",
      "\n",
      "Adverserial loss: 0.76372147\n",
      "\n",
      "Discriminator loss: 0.6928476\n",
      "\n",
      "Adverserial loss: 0.75884837\n",
      "\n",
      "Discriminator loss: 0.6932778\n",
      "\n",
      "Adverserial loss: 0.7470001\n",
      "\n",
      "Discriminator loss: 0.6711348\n",
      "\n",
      "Adverserial loss: 0.5239169\n",
      "\n",
      "Discriminator loss: 0.6738021\n",
      "\n",
      "Adverserial loss: 0.77150345\n",
      "\n",
      "Discriminator loss: 0.70803756\n",
      "\n",
      "Adverserial loss: 0.7466348\n",
      "\n",
      "Discriminator loss: 0.6788064\n",
      "\n",
      "Adverserial loss: 0.75530744\n",
      "\n",
      "Discriminator loss: 0.69719917\n",
      "\n",
      "Adverserial loss: 0.75796074\n",
      "\n",
      "Discriminator loss: 0.6899656\n",
      "\n",
      "Adverserial loss: 0.7647645\n",
      "\n",
      "Discriminator loss: 0.70079017\n",
      "\n",
      "Adverserial loss: 0.79018706\n",
      "\n",
      "Discriminator loss: 0.71016186\n",
      "\n",
      "Adverserial loss: 0.76590073\n",
      "\n",
      "Discriminator loss: 0.6857459\n",
      "\n",
      "Adverserial loss: 0.6724707\n",
      "\n",
      "Discriminator loss: 0.7059579\n",
      "\n",
      "Adverserial loss: 0.7171316\n",
      "\n",
      "Discriminator loss: 0.6981747\n",
      "\n",
      "Adverserial loss: 0.73340833\n",
      "\n",
      "Discriminator loss: 0.6933824\n",
      "\n",
      "Adverserial loss: 0.7583738\n",
      "\n",
      "Discriminator loss: 0.71759826\n",
      "\n",
      "Adverserial loss: 0.78922176\n",
      "\n",
      "Discriminator loss: 0.6874156\n",
      "\n",
      "Adverserial loss: 0.73807955\n",
      "\n",
      "Discriminator loss: 0.7032024\n",
      "\n",
      "Adverserial loss: 0.73351526\n",
      "\n",
      "Discriminator loss: 0.6860583\n",
      "\n",
      "Adverserial loss: 0.71842974\n",
      "\n",
      "Discriminator loss: 0.7011434\n",
      "\n",
      "Adverserial loss: 0.718027\n",
      "\n",
      "Discriminator loss: 0.6685742\n",
      "\n",
      "Adverserial loss: 0.841399\n",
      "\n",
      "Discriminator loss: 0.7312674\n",
      "\n",
      "Adverserial loss: 0.98277634\n",
      "\n",
      "Discriminator loss: 0.70599353\n",
      "\n",
      "Adverserial loss: 0.863704\n",
      "\n",
      "Discriminator loss: 0.6995827\n",
      "\n",
      "Adverserial loss: 0.7206665\n",
      "\n",
      "Discriminator loss: 0.6867205\n",
      "\n",
      "Adverserial loss: 0.7391301\n",
      "\n",
      "Discriminator loss: 0.82951945\n",
      "\n",
      "Adverserial loss: 1.1979778\n",
      "\n",
      "Discriminator loss: 0.6830025\n",
      "\n",
      "Adverserial loss: 0.72588366\n",
      "\n",
      "Discriminator loss: 0.66577446\n",
      "\n",
      "Adverserial loss: 0.7702715\n",
      "\n",
      "Discriminator loss: 0.67456245\n",
      "\n",
      "Adverserial loss: 0.7450224\n",
      "\n",
      "Discriminator loss: 0.684217\n",
      "\n",
      "Adverserial loss: 0.7853586\n",
      "\n",
      "Discriminator loss: 0.7161748\n",
      "\n",
      "Adverserial loss: 0.86255205\n",
      "\n",
      "Discriminator loss: 0.7029079\n",
      "\n",
      "Adverserial loss: 0.8545238\n",
      "\n",
      "Discriminator loss: 0.6623451\n",
      "\n",
      "Adverserial loss: 0.8235491\n",
      "\n",
      "Discriminator loss: 0.6897141\n",
      "\n",
      "Adverserial loss: 0.8152276\n",
      "\n",
      "Discriminator loss: 0.64770216\n",
      "\n",
      "Adverserial loss: 0.786086\n",
      "\n",
      "Discriminator loss: 0.7128194\n",
      "\n",
      "Adverserial loss: 0.7945898\n",
      "\n",
      "Discriminator loss: 0.6924365\n",
      "\n",
      "Adverserial loss: 0.7408505\n",
      "\n",
      "Discriminator loss: 0.6892859\n",
      "\n",
      "Adverserial loss: 0.7956651\n",
      "\n",
      "Discriminator loss: 0.6740837\n",
      "\n",
      "Adverserial loss: 0.97105503\n",
      "\n",
      "Discriminator loss: 0.6953689\n",
      "\n",
      "Adverserial loss: 0.77638745\n",
      "\n",
      "Discriminator loss: 0.6848096\n",
      "\n",
      "Adverserial loss: 0.7495963\n",
      "\n",
      "Discriminator loss: 0.686172\n",
      "\n",
      "Adverserial loss: 0.8165536\n",
      "\n",
      "Discriminator loss: 0.6768028\n",
      "\n",
      "Adverserial loss: 0.9000066\n",
      "\n",
      "Discriminator loss: 0.6630945\n",
      "\n",
      "Adverserial loss: 0.987429\n",
      "\n",
      "Discriminator loss: 0.67509615\n",
      "\n",
      "Adverserial loss: 0.7630235\n",
      "\n",
      "Discriminator loss: 0.7232324\n",
      "\n",
      "Adverserial loss: 0.8115443\n",
      "\n",
      "Discriminator loss: 0.7824657\n",
      "\n",
      "Adverserial loss: 1.177256\n",
      "\n",
      "Discriminator loss: 0.7118297\n",
      "\n",
      "Adverserial loss: 0.73408353\n",
      "\n",
      "Discriminator loss: 0.7435507\n",
      "\n",
      "Adverserial loss: 1.0147959\n",
      "\n",
      "Discriminator loss: 0.6911133\n",
      "\n",
      "Adverserial loss: 0.8148147\n",
      "\n",
      "Discriminator loss: 0.67225856\n",
      "\n",
      "Adverserial loss: 0.9408493\n",
      "\n",
      "Discriminator loss: 0.66766316\n",
      "\n",
      "Adverserial loss: 0.8106965\n",
      "\n",
      "Discriminator loss: 1.2058837\n",
      "\n",
      "Adverserial loss: 0.8826038\n",
      "\n",
      "Discriminator loss: 0.6700202\n",
      "\n",
      "Adverserial loss: 0.83523667\n",
      "\n",
      "Discriminator loss: 0.7106929\n",
      "\n",
      "Adverserial loss: 0.88812983\n",
      "\n",
      "Discriminator loss: 0.73759\n",
      "\n",
      "Adverserial loss: 0.7102909\n",
      "\n",
      "Discriminator loss: 0.6858556\n",
      "\n",
      "Adverserial loss: 1.04861\n",
      "\n",
      "Discriminator loss: 0.6556902\n",
      "\n",
      "Adverserial loss: 0.8655378\n",
      "\n",
      "Discriminator loss: 0.65970355\n",
      "\n",
      "Adverserial loss: 0.8072321\n",
      "\n",
      "Discriminator loss: 0.66531485\n",
      "\n",
      "Adverserial loss: 0.83408797\n",
      "\n",
      "Discriminator loss: 0.72258425\n",
      "\n",
      "Adverserial loss: 0.8132434\n",
      "\n",
      "Discriminator loss: 0.68000203\n",
      "\n",
      "Adverserial loss: 0.73632234\n",
      "\n",
      "Discriminator loss: 0.7085297\n",
      "\n",
      "Adverserial loss: 0.79351133\n"
     ]
    }
   ],
   "source": [
    "iterations=10000\n",
    "batch_size=20\n",
    "save_dir='./gan/'\n",
    "\n",
    "start=0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors=np.random.normal(size=(batch_size,latent_dim))\n",
    "    generated_images=generator.predict(random_latent_vectors)\n",
    "    \n",
    "    stop=start+batch_size\n",
    "    real_images=x_train[start:stop]\n",
    "    combined_images=np.concatenate([generated_images,real_images])\n",
    "    \n",
    "    labels=np.concatenate([np.ones((batch_size,1)),np.zeros((batch_size,1))])\n",
    "    labels+=0.05*np.random.random(labels.shape)\n",
    "    \n",
    "    d_loss=discriminator.train_on_batch(combined_images,labels)\n",
    "    random_latent_vectors=np.random.normal(size=(batch_size,latent_dim))\n",
    "    misleading_tragets=np.zeros((batch_size,1))\n",
    "    a_loss=gan.train_on_batch(random_latent_vectors,misleading_tragets)\n",
    "    \n",
    "    start+=batch_size\n",
    "    if start>len(x_train)-batch_size:\n",
    "        start=0\n",
    "    if step%100==0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print('\\nDiscriminator loss:',d_loss)\n",
    "        print('\\nAdverserial loss:',a_loss)\n",
    "        \n",
    "        img=image.array_to_img(generated_images[0]*255.,scale=False)\n",
    "        img.save(os.path.join(save_dir,'generated'+str(step)+'.png'))\n",
    "        \n",
    "        img=image.array_to_img(real_images[0]*255.,scale=False)\n",
    "        img.save(os.path.join(save_dir,'real'+str(step)+'.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
